{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAYtdnGtgwIj"
      },
      "outputs": [],
      "source": [
        "!pip install -q loralib\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpMnDXyzhEUt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7HGY8U2hsYe"
      },
      "outputs": [],
      "source": [
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wNJUq_9hlmA"
      },
      "outputs": [],
      "source": [
        "assert (\n",
        "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
        "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a388Hut_hIo_"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"dominguesm/alpaca-lora-ptbr-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pdk1Gugji2na"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa, emparelhada com uma entrada que fornece mais contexto. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{instruction}\n",
        "\n",
        "### Entrada:\n",
        "{input}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Abaixo está uma instrução que descreve uma tarefa. Escreva uma resposta que conclua adequadamente a solicitação.\n",
        "\n",
        "### Instruções:\n",
        "{instruction}\n",
        "\n",
        "### Resposta:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS8rZH7citYr"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYoPyVPOijIN"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        pprint(\"Resposta: \" + output.split(\"### Resposta:\")[1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8eCQ_Tt-fvt",
        "outputId": "231a85f6-ad24-4907-be35-6051b76bc1d3"
      },
      "outputs": [],
      "source": [
        "evaluate(input(\"Instrução: \"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
